{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "with open ('../params.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "table_prefix = params['table_prefix']\n",
    "diseases = params['diseases']\n",
    "case_limit = params['case_limit']\n",
    "control_limit = params['control_limit']\n",
    "min_enrollment = params['enrollment']\n",
    "user = params['user']\n",
    "\n",
    "chunk = True\n",
    "\n",
    "creds_file = \"/home/\" + user + \"/creds.txt\" \n",
    "creds = lines = [line.rstrip('\\n') for line in open(creds_file)]\n",
    "\n",
    "connection_string = (\"Driver={ODBC Driver 17 for SQL Server};\" + \n",
    "                     \"server=\" + creds[0] + \";\" +\n",
    "                     \"domain=\" + creds[1] + \";\" +  \n",
    "                     \"database=\" + creds[2] + \";\" +\n",
    "                     \"uid=\" + creds[3]  + \";\" +\n",
    "                     \"pwd=\" + creds[4] + \";\" +\n",
    "                     \"ssl=require;\")\n",
    "\n",
    "cn = pyodbc.connect(connection_string, autocommit=True)\n",
    "cursor = cn.cursor()\n",
    "\n",
    "# directory = '../../data/' + str(table_prefix) + '_' + str(case_limit)\n",
    "directory = '../../data/diseaes_replaced' + str(table_prefix) + '_' + str(case_limit)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all members\n",
    "sql = \"SELECT DISTINCT(MemberNum) FROM \" + user + \".dbo.\" + table_prefix +\\\n",
    "      \"_sequences ORDER BY MemberNum\"\n",
    "members = pd.read_sql_query(sql, cn)\n",
    "print(members.shape)\n",
    "members.to_csv(directory + '/members.csv', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get all to generate dictionary / popular terms etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle as pkl\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"SELECT MemberNum, fromIndex, Type, Object FROM \" + user + \".dbo.\" + table_prefix + \"_sequences ORDER BY MemberNum, Date, TYPE\"\n",
    "\n",
    "if chunk:\n",
    "    chunk_list = []\n",
    "    i = 0\n",
    "    for c in pd.read_sql_query(sql , cn, chunksize=10000000):\n",
    "        print(i, time.time() - start_time)\n",
    "        chunk_list.append(c)\n",
    "        i += 1\n",
    "else:\n",
    "    seq = pd.read_sql(sql, cn)\n",
    "    \n",
    "print(time.time() - start_time)\n",
    "%time seq = pd.concat(chunk_list)\n",
    "print(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seq[np.isfinite(seq['fromIndex'])]\n",
    "print(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq['Type'] = seq['Type'].astype('int64')\n",
    "seq['fromIndex'] = seq['fromIndex'].astype(int)\n",
    "seq['Object'] = seq['Object'].astype(str)\n",
    "seq['Type_obj'] = seq['Type'].astype(str) + '_' + seq['Object'].astype(str)\n",
    "\n",
    "val_counts = seq['Type_obj'].value_counts()\n",
    "val_counts = val_counts.sort_values(ascending=False).reset_index()\n",
    "val_counts.columns = ['Type_obj', 'count']\n",
    "\n",
    "print(val_counts.head())\n",
    "\n",
    "# common dict\n",
    "common_dict = seq[seq['Type'] == 0]['Object'].value_counts().head(n=500).to_dict()\n",
    "i = 0\n",
    "for key in common_dict:\n",
    "    common_dict[key] = i\n",
    "    i+=1\n",
    "pkl.dump(common_dict, open(directory + '/common_dict.pkl', 'wb'))\n",
    "\n",
    "# full dict\n",
    "dictionary = {}\n",
    "for i, val in val_counts.iterrows():\n",
    "    dictionary[i] = val['Type_obj'].strip()\n",
    "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "pkl.dump(dictionary, open(directory + '/full_dict.pkl', 'wb'))\n",
    "val_counts[1000:].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dictionary), len(reversed_dictionary))\n",
    "seq['Type_obj'] = seq['Type_obj'].str.strip()\n",
    "seq['Type_obj'] = seq['Type_obj'].replace(to_replace=['0_332.0', '0_332.1'], value='dis')\n",
    "%time sentences = seq.groupby('MemberNum')['Type_obj'].apply(list).tolist()\n",
    "\n",
    "# sentences\n",
    "# size - dimensionality of feature vectors\n",
    "# window - max distance between current and predicted word\n",
    "# min count - ignore rare words (should be higher than 5)\n",
    "print(len(sentences))\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=(len(sentences)*0.1),\n",
    "                 workers=16)\n",
    "model.save(directory + '/w2v_' + user + '_' + table_prefix + '.model')\n",
    "print(len(model.wv.vocab))\n",
    "\n",
    "val_counts = val_counts[:len(model.wv.vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shard_size = 10\n",
    "fixed_vocab = val_counts.shape[0]\n",
    "input_codes = sorted(list(val_counts['Type_obj']))\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_codes)])\n",
    "print('Vocab size', fixed_vocab, 'input_heads', len(input_codes))\n",
    "pkl.dump(input_token_index, open(directory + '/input_token_index.pkl', 'wb'))\n",
    "\n",
    "# @TODO add common diseases and remove too close to end\n",
    "# @ TODO replace diseases with 'disease'\n",
    "for i in range(members.shape[0] / shard_size):\n",
    "    print(str(i) + '/' + str(members.shape[0] / shard_size))\n",
    "    include_members = members[i*shard_size:(i+1)*shard_size]\n",
    "    query_param = (include_members['MemberNum'].astype(str).str.cat(sep=\", \"))\n",
    "    \n",
    "    sql = (\"SELECT MemberNum, fromIndex, Type, Object FROM \" + user +\\\n",
    "           \".dbo.\" + table_prefix + \"_sequences \" +\\\n",
    "           \"WHERE MemberNum in (\" + query_param + \")\" +\\\n",
    "           \"ORDER BY MemberNum, cast(fromIndex as int) ASC, Type, Object\")\n",
    "    #print(sql)\n",
    "    seq = pd.read_sql(sql, cn)\n",
    "    seq['Type_obj'] = seq['Type'].astype(str) + '_' + seq['Object'].astype(str)\n",
    "    seq['Type_obj'] = seq['Type_obj'].replace(to_replace=['0_332.0', '0_332.1'], value='dis')\n",
    "    print(seq.shape)\n",
    "    \n",
    "    # reduce size - only words in vocab\n",
    "    seqFiltered = seq[seq['Type_obj'].isin(model.wv.vocab)]\n",
    "    #print(seqFiltered.shape)\n",
    "    \n",
    "    seqFiltered['InputCode'] = seqFiltered['Type_obj'].map(input_token_index)\n",
    "    seqFiltered = seqFiltered[seqFiltered['InputCode'].notnull()]\n",
    "    #print(seqFiltered.shape)\n",
    "    \n",
    "    seqFiltered.reset_index(inplace=True, drop=True)\n",
    "    seq_del = seqFiltered.copy(deep=True)\n",
    "    \n",
    "    for memberNum in seqFiltered['MemberNum'].unique():\n",
    "        member = seqFiltered.loc[(seqFiltered['MemberNum'] == memberNum)]\\\n",
    "                            .sort_values(by=['fromIndex', 'Type', 'Object'], ascending=True)\n",
    "\n",
    "        firstRow = member.index.min()\n",
    "        index_minus25 = member[(member['fromIndex'] == 0)].index.min() - 24\n",
    "        lastRow = member.index.max()\n",
    "\n",
    "        if firstRow < index_minus25:\n",
    "            seq_del.drop(seqFiltered.index[firstRow: index_minus25], inplace=True)\n",
    "        if (index_minus25 <= 0) or (firstRow > lastRow-25):\n",
    "            seq_del.drop(seqFiltered.index[firstRow: lastRow], inplace=True)\n",
    "            \n",
    "    print(seq_del.shape)\n",
    "    seq_del.to_csv(directory + '/seq_' + str(i*shard_size) + \"_\" + \n",
    "                   str((i+1) * shard_size) + '.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
